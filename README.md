# Do LMMs’ language obey the Zipf's law? An exploratory analysis. 

## Philosophy
***
### Introduction
According to Zipf's law, _if a set of measured values is sorted in decreasing order, the value of the n-th entry is often approximately inversely proportional to n_. Since Zipf’s law holds for human-generated text and speech, one can ask whether the law is encoded in a way in which we use the language to refer to external entities or it emerges from the intrinsic properties of language such as syntactic relations between signs. One of the ways of examining this question is measuring Zipf's law  properties in purely relational usage of language employed by LLMs. To be precise: we assume that LLMs’s  semantics can be described as nonextensional, usage-based characterization of meaning as the relations between particular words. Thus, it is an interesting question to examine whether Zipf’s law describes the way in which LLMs use language properly. 

In conducting an analysis of the entropy present in prompts and responses, we will essentially be reconstructing the experiments run by Shannon (1951) and Moradi, Grzymala-Busse, and Roberts (1998), albeit with a more advanced AI model for text prediction. This analysis will give us insight into the entropy of the English language, the entropy-reducing power of the LMM model, and at least a qualitative examination of the correlation between entropy and semantic content. We would like to strongly emphasise, that the project is conducted as an exploratory analysis.

 The purpose of the project is threefold:
- Provide an exploratory analysis of the word-occurrences in LLM’s language 
- Measure the entropy between source-data and LMM’s response, to check whether the LMM provides a sufficient amount of information to the receiver
- Examine whether or not the language generated by LMM’s obeys Zipf’s law

### Characterization of the dataset.
The dataset is a recording of a series of conversations generated by LLaMA 3.1 70B model, which was trained on the Stanford Encyclopedia of Philosophy. It consists of 11904 conversations within the rage of 2 to 38 messages on various advancement levels (undergraduate, master's, PhD). 

### Aims of the analysis.
In our analysis, we aim to employ the steps in Python characterized below: 
- Exploratory analysis of the text data (e.g. Number of characters in each utterance, Distribution of Average Word Length per Utterance ect.)
- LDA topic modelling 
- Compute shannonian entropies for each prompt and response
- Computing Zipf’s law for the set of LLaMA responses to check whether the law is a proper description of LLaMA’s way of using the language

***
## Computer science

### Installation and requirements
To use spacy, you have to install the **spacy** package (via pip) 

`pip install spacy`

and download **en_core_web_sm** spacy model (tokenization)

`python -m spacy download en_core_web_sm`

Other dependencies can be downloaded with this command:

`pip install -r /your/path/to/requirements.txt`

### Sources of code
#### Online 
- https://www.geeksforgeeks.org/python/extract-dictionary-value-from-column-in-data-frame/
- https://www.geeksforgeeks.org/python/how-to-add-empty-column-to-dataframe-in-pandas/
#### GitHub repositories
- https://github.com/CodeDrome/zipfs-law-python